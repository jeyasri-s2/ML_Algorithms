{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidian_distance(point1, point2):\n",
    "    return np.sqrt(np.sum((np.array(point2) - np.array(point1))**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_predict(training_data, training_labels, test_point, k):\n",
    "    distances = []\n",
    "    \n",
    "    for i in range(len(training_data)):\n",
    "        distances.append((euclidian_distance(training_data[i], test_point), training_labels[i]))\n",
    "    distances.sort(key=lambda x:x[0])\n",
    "    k_nearest_neighbors = [label for _,label in distances[:k]]\n",
    "    return Counter(k_nearest_neighbors).most_common(1)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = [[1, 2], [2, 3], [3, 4], [6, 7], [7, 8]]\n",
    "training_labels = ['A', 'A', 'A', 'B', 'B']\n",
    "test_point = [7, 9]\n",
    "k = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B\n"
     ]
    }
   ],
   "source": [
    "prediction = knn_predict(training_data, training_labels, test_point, k)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the Euclidean distance\n",
    "def euclidean_distance(x1, x2):\n",
    "    return np.sqrt(np.sum((x1 - x2) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal value of k: 1\n",
      "Best accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "class KNN:\n",
    "    def __init__(self, k=3):\n",
    "        self.k = k\n",
    "        \n",
    "    def fit(self,X_train, y_train):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        \n",
    "    def predict(self, x_test):\n",
    "        predictions = [self.predict_(x) for x in x_test]\n",
    "        return np.array(predictions)\n",
    "    \n",
    "    def predict_(self, x):\n",
    "        distances = [euclidean_distance(x, x_train) for x_train in self.X_train]\n",
    "        k_indices = np.argsort(distances)[:self.k]  \n",
    "        k_nearest_labels = [self.y_train[i] for i in k_indices]\n",
    "        most_common = Counter(k_nearest_labels).most_common(1)\n",
    "        return most_common[0][0]\n",
    "    \n",
    "def find_optimal_k(X, y, max_k=20):\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    best_k = None\n",
    "    best_accuracy = 0\n",
    "\n",
    "    # Try different values of k\n",
    "    for k in range(1, max_k + 1):\n",
    "        # Initialize and train KNN model\n",
    "        knn = KNN(k=k)\n",
    "        knn.fit(X_train, y_train)\n",
    "\n",
    "        # Make predictions on the validation set\n",
    "        predictions = knn.predict(X_val)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        accuracy = np.mean(predictions == y_val)\n",
    "\n",
    "        # Update the best k if we found a better accuracy\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_k = k\n",
    "\n",
    "    return best_k, best_accuracy\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample data (2D data for simplicity)\n",
    "    X = np.array([[1, 2], [2, 3], [3, 4], [5, 7], [7, 8], [8, 9], [1, 1], [2, 2], [3, 3], [4, 4]])\n",
    "    y = np.array([0, 0, 1, 1, 0, 1, 0, 0, 1, 1])  # Class labels\n",
    "\n",
    "    # Find optimal k\n",
    "    optimal_k, best_accuracy = find_optimal_k(X, y, max_k=5)\n",
    "\n",
    "    print(f\"Optimal value of k: {optimal_k}\")\n",
    "    print(f\"Best accuracy: {best_accuracy}\")\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Methods for Selecting k:\n",
    "\n",
    "* **Cross-Validation:** A robust method for selecting the best k is to perform k-fold cross-validation. This involves splitting the data into k subsets training the model on some subsets and testing it on the remaining ones and repeating this for each subset. The value of k that results in the highest average validation accuracy is usually the best choice.\n",
    "* **Elbow Method:** In the elbow method we plot the model’s error rate or accuracy for different values of k. As we increase k the error usually decreases initially. However after a certain point the error rate starts to decrease more slowly. This point where the curve forms an “elbow” that point is considered as best k.\n",
    "* **Odd Values for k:** It’s also recommended to choose an odd value for k especially in classification tasks to avoid ties when deciding the majority class."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
