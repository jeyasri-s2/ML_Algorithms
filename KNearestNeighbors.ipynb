{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidian_distance(point1, point2):\n",
    "    return np.sqrt(np.sum((np.array(point2) - np.array(point1))**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_predict(training_data, training_labels, test_point, k):\n",
    "    distances = []\n",
    "    \n",
    "    for i in range(len(training_data)):\n",
    "        distances.append((euclidian_distance(training_data[i], test_point), training_labels[i]))\n",
    "    distances.sort(key=lambda x:x[0])\n",
    "    k_nearest_neighbors = [label for _,label in distances[:k]]\n",
    "    return Counter(k_nearest_neighbors).most_common(1)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = [[1, 2], [2, 3], [3, 4], [6, 7], [7, 8]]\n",
    "training_labels = ['A', 'A', 'A', 'B', 'B']\n",
    "test_point = [7, 9]\n",
    "k = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B\n"
     ]
    }
   ],
   "source": [
    "prediction = knn_predict(training_data, training_labels, test_point, k)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the Euclidean distance\n",
    "def euclidean_distance(x1, x2):\n",
    "    return np.sqrt(np.sum((x1 - x2) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal value of k: 1\n",
      "Best accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "class KNN:\n",
    "    def __init__(self, k=3):\n",
    "        self.k = k\n",
    "        \n",
    "    def fit(self,X_train, y_train):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        \n",
    "    def predict(self, x_test):\n",
    "        predictions = [self.predict_(x) for x in x_test]\n",
    "        return np.array(predictions)\n",
    "    \n",
    "    def predict_(self, x):\n",
    "        distances = [euclidean_distance(x, x_train) for x_train in self.X_train]\n",
    "        k_indices = np.argsort(distances)[:self.k]  \n",
    "        k_nearest_labels = [self.y_train[i] for i in k_indices]\n",
    "        most_common = Counter(k_nearest_labels).most_common(1)\n",
    "        return most_common[0][0]\n",
    "    \n",
    "def find_optimal_k(X, y, max_k=20):\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    best_k = None\n",
    "    best_accuracy = 0\n",
    "\n",
    "    # Try different values of k\n",
    "    for k in range(1, max_k + 1):\n",
    "        # Initialize and train KNN model\n",
    "        knn = KNN(k=k)\n",
    "        knn.fit(X_train, y_train)\n",
    "\n",
    "        # Make predictions on the validation set\n",
    "        predictions = knn.predict(X_val)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        accuracy = np.mean(predictions == y_val)\n",
    "\n",
    "        # Update the best k if we found a better accuracy\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_k = k\n",
    "\n",
    "    return best_k, best_accuracy\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample data (2D data for simplicity)\n",
    "    X = np.array([[1, 2], [2, 3], [3, 4], [5, 7], [7, 8], [8, 9], [1, 1], [2, 2], [3, 3], [4, 4]])\n",
    "    y = np.array([0, 0, 1, 1, 0, 1, 0, 0, 1, 1])  # Class labels\n",
    "\n",
    "    # Find optimal k\n",
    "    optimal_k, best_accuracy = find_optimal_k(X, y, max_k=5)\n",
    "\n",
    "    print(f\"Optimal value of k: {optimal_k}\")\n",
    "    print(f\"Best accuracy: {best_accuracy}\")\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to choose the value of k for KNN Algorithm?\n",
    "The value of k is critical in KNN as it determines the number of neighbors to consider when making predictions. Selecting the optimal value of k depends on the characteristics of the input data. If the dataset has significant outliers or noise a higher k can help smooth out the predictions and reduce the influence of noisy data. However choosing very high value can lead to underfitting where the model becomes too simplistic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Methods for Selecting k:\n",
    "\n",
    "* **Cross-Validation:** A robust method for selecting the best k is to perform k-fold cross-validation. This involves splitting the data into k subsets training the model on some subsets and testing it on the remaining ones and repeating this for each subset. The value of k that results in the highest average validation accuracy is usually the best choice.\n",
    "* **Elbow Method:** In the elbow method we plot the model’s error rate or accuracy for different values of k. As we increase k the error usually decreases initially. However after a certain point the error rate starts to decrease more slowly. This point where the curve forms an “elbow” that point is considered as best k.\n",
    "* **Odd Values for k:** It’s also recommended to choose an odd value for k especially in classification tasks to avoid ties when deciding the majority class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfitting and Underfitting\n",
    "\n",
    "* **Overfitting** : The model prefectly memorizes the whole dataset but doesnot clearly separates the two classes. Thie might be induced by the fact that we are retaining all the observed training noise, and therefore it is difficult to generalize.\n",
    "* **Underfitting** : The model is too simple, and it is not able to extract useful information from the training set. The accuracy of training and test ste will be similar and tend to be smaller as the model gets simpler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation :\n",
    "\n",
    "Instead of splitting data to 3 folds - training, test and validation. We split the data to 'n' folds. \n",
    "We pick one fold as test set and the remaining (n-1) fold will be used to fit the model (training set). The process is repeated for all n , non-overlapping different folds, obtaining n different scores.\n",
    "\n",
    "you cross validate the training set to look for best features and how the choosen configuration of parameters will perform on new data.\n",
    "\n",
    "Use \"Cross-validation score\", pick the best score and pick model parameters associated with it ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All cross_val_scores  [0.9523809523809523, 0.9333333333333333, 0.9428571428571428, 0.9523809523809523, 0.9428571428571428, 0.9333333333333333, 0.9428571428571428]\n",
      "Best CV Score: 0.9524\n",
      "Best n_neighbors: 1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data  # features\n",
    "y = iris.target  # target labels\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=42)\n",
    "cross_val_scores = []\n",
    "neighbors = range(1,15,2)\n",
    "for neighbor in range(1,15,2): # use only old values for neighbors\n",
    "    knn = KNeighborsClassifier(n_neighbors=neighbor)\n",
    "    scores = cross_val_score(knn, X_train, y_train, cv=5)\n",
    "    cross_val_scores.append(np.mean(scores))\n",
    "\n",
    "print(\"All cross_val_scores \",cross_val_scores)\n",
    "print(\"Best CV Score: {:.4f}\".format(np.max(cross_val_scores)))\n",
    "best_nn = neighbors[np.argmax(cross_val_scores)]\n",
    "print(\"Best n_neighbors: {}\".format(best_nn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid search CV\n",
    "\n",
    "**GridSearchCV** is a technique used to **tune the hyperparameters of a machine learning model** to find the best combination of parameters that maximizes model performance. It performs an exhaustive search over a specified parameter grid and **evaluates each combination using cross-validation**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found: {'kneighborsclassifier__n_neighbors': 7, 'kneighborsclassifier__p': 2, 'kneighborsclassifier__weights': 'distance'}\n",
      "Best cross-validation score: 0.9666666666666666\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data  # features\n",
    "y = iris.target  # target labels\n",
    "\n",
    "# Create a pipeline that standardizes the data and applies KNN\n",
    "pipeline = make_pipeline(StandardScaler(), KNeighborsClassifier())\n",
    "\n",
    "# Define a parameter grid to search over\n",
    "param_grid = {\n",
    "    'kneighborsclassifier__n_neighbors': [1, 3, 5, 7, 9],  # KNN hyperparameters\n",
    "    'kneighborsclassifier__weights': ['uniform', 'distance'],  # Weighting strategy\n",
    "    'kneighborsclassifier__p': [1, 2]  # Distance metric (1: Manhattan, 2: Euclidean)\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV with 5-fold cross-validation\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5)\n",
    "\n",
    "# Fit GridSearchCV to the data\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Print the best parameters and the corresponding score\n",
    "print(f\"Best parameters found: {grid_search.best_params_}\")\n",
    "print(f\"Best cross-validation score: {grid_search.best_score_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RandomizedSearchCV** is another hyperparameter tuning technique, similar to GridSearchCV, but with one major difference: instead of searching through every possible combination of hyperparameters (like grid search), RandomizedSearchCV randomly samples a fixed number of combinations from the specified hyperparameter space.\n",
    "\n",
    "This allows RandomizedSearchCV to explore a broader range of hyperparameters in less time, making it especially useful when dealing with large search spaces or computationally expensive models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Differences Between GridSearchCV and RandomizedSearchCV:\n",
    "\n",
    "* GridSearchCV exhaustively searches through every combination of hyperparameters in a predefined grid.\n",
    "* RandomizedSearchCV randomly samples a fixed number of combinations of hyperparameters. The number of iterations (or combinations) is set by the user and can be controlled with the n_iter parameter.\n",
    "\n",
    "### Benefits of RandomizedSearchCV:\n",
    "* **Faster:** It is faster than GridSearchCV, especially when the hyperparameter space is large, because it does not try every possible combination.\n",
    "* **Broader exploration:** Randomized search can explore more hyperparameter combinations (within a fixed number of iterations) than grid search.\n",
    "* **Scalability:** Ideal for large datasets or when hyperparameter tuning is computationally expensive.\n",
    "\n",
    "### Workflow of RandomizedSearchCV:\n",
    "* **Define a distribution for each hyperparameter:** Specify ranges or distributions of values for the hyperparameters.\n",
    "* **Randomly sample** a set number of combinations from the search space.\n",
    "* **Perform cross-validation:** For each random combination, cross-validation is performed to evaluate its performance.\n",
    "* **Select the best hyperparameters** based on the performance of the cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found: {'kneighborsclassifier__n_neighbors': 8, 'kneighborsclassifier__p': 2, 'kneighborsclassifier__weights': 'uniform'}\n",
      "Best cross-validation score: 0.9666666666666668\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from scipy.stats import randint\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data  # features\n",
    "y = iris.target  # target labels\n",
    "\n",
    "# Create a pipeline that standardizes the data and applies KNN\n",
    "pipeline = make_pipeline(StandardScaler(), KNeighborsClassifier())\n",
    "\n",
    "# Define a hyperparameter distribution to sample from\n",
    "param_dist = {\n",
    "    'kneighborsclassifier__n_neighbors': randint(1, 20),  # Random integer between 1 and 20\n",
    "    'kneighborsclassifier__weights': ['uniform', 'distance'],  # Weighting strategy\n",
    "    'kneighborsclassifier__p': [1, 2]  # Distance metric (1: Manhattan, 2: Euclidean)\n",
    "}\n",
    "\n",
    "# Set up RandomizedSearchCV with 5-fold cross-validation and 100 iterations\n",
    "random_search = RandomizedSearchCV(pipeline, param_dist, n_iter=100, cv=5, random_state=42)\n",
    "\n",
    "# Fit RandomizedSearchCV to the data\n",
    "random_search.fit(X, y)\n",
    "\n",
    "# Print the best parameters and the corresponding score\n",
    "print(f\"Best parameters found: {random_search.best_params_}\")\n",
    "print(f\"Best cross-validation score: {random_search.best_score_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores for each fold: [0.96666667 0.96666667 0.93333333 0.9        1.        ]\n",
      "Average cross-validation score: 0.9533333333333334\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data  # features\n",
    "y = iris.target  # target labels\n",
    "\n",
    "# Create a pipeline that standardizes the data and applies KNN\n",
    "pipeline = make_pipeline(StandardScaler(), KNeighborsClassifier(n_neighbors=3))\n",
    "\n",
    "# Perform 5-fold cross-validation\n",
    "cv_scores = cross_val_score(pipeline, X, y, cv=5)\n",
    "\n",
    "# Print the cross-validation scores\n",
    "print(f\"Cross-validation scores for each fold: {cv_scores}\")\n",
    "print(f\"Average cross-validation score: {np.mean(cv_scores)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation\n",
    "\n",
    "* KNeighborsClassifier(n_neighbors=3): This initializes the KNN classifier with 3 neighbors.\n",
    "* StandardScaler(): It's important to standardize the data because KNN is sensitive to the scale of the features.\n",
    "* cross_val_score(): It performs 5-fold cross-validation, splitting the data into 5 parts and evaluating the model's performance on each fold.\n",
    "* cv=5: This parameter specifies that we are performing 5-fold cross-validation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
