{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bootstrap Aggregation**, commonly known as **Bagging**, is an ensemble learning technique that is primarily used to improve the accuracy and robustness of machine learning models, especially for **unstable models** like decision trees. It involves training multiple instances of a model on **randomly sampled subsets** of the training data and then combining their predictions. The main idea behind Bagging is to reduce variance by averaging out errors across multiple models, which helps in preventing overfitting.\n",
    "\n",
    "### Key Concepts:\n",
    "\n",
    "1. **Bootstrapping**: This refers to the process of randomly selecting **subsets of data with replacement**. For each new model in the ensemble, a different bootstrap sample (subset) is drawn from the training data. Since sampling is done with replacement, some examples from the original training set may appear multiple times in a bootstrap sample, while others may not appear at all.\n",
    "\n",
    "2. **Aggregation**: Once all the models are trained, the final prediction is made by aggregating the predictions of all individual models. This could be done using:\n",
    "   - **For Regression**: The average of all predictions is taken.\n",
    "   - **For Classification**: The majority vote (the class that appears the most often) is selected.\n",
    "\n",
    "### Why Bagging Works:\n",
    "- **Variance Reduction**: By training multiple models on different subsets of data, Bagging helps to reduce the model's variance, which results in better performance and more stable predictions. It is especially useful for **high-variance, low-bias models** (e.g., decision trees).\n",
    "- **Improved Generalization**: Combining predictions from multiple models generally leads to better generalization to unseen data.\n",
    "  \n",
    "### Steps Involved in Bagging:\n",
    "1. **Bootstrap Sampling**: Create multiple random samples (with replacement) from the original training dataset. Each sample is used to train an individual model.\n",
    "2. **Model Training**: Train a model (e.g., a decision tree) on each of the bootstrap samples.\n",
    "3. **Aggregation of Predictions**: Combine the predictions of all the models to make a final prediction.\n",
    "\n",
    "---\n",
    "\n",
    "### Bagging Example with Decision Trees:\n",
    "- In the case of **Bagging with Decision Trees** (one of the most common examples), the steps would be as follows:\n",
    "  1. Randomly select multiple bootstrap samples of the dataset.\n",
    "  2. Train a decision tree on each sample. Since decision trees are prone to overfitting, each tree will be slightly different because each sees a different subset of the data.\n",
    "  3. When making a prediction, take the majority vote (for classification) or the average (for regression) of all the decision trees.\n",
    "\n",
    "\n",
    "\n",
    "### Key Parameters in `BaggingClassifier`:\n",
    "- **base_estimator**: The model to use as the base estimator (default is `DecisionTreeClassifier`).\n",
    "- **n_estimators**: The number of base models (e.g., the number of decision trees) to train. More estimators usually result in better performance, but at the cost of increased computation.\n",
    "- **max_samples**: The number of samples to use for each model. If set to `1.0`, all samples are used, and if set to a fraction, only that fraction of samples are used.\n",
    "- **max_features**: The number of features to use for each model. Helps to diversify the base models.\n",
    "- **random_state**: Ensures reproducibility by controlling random number generation.\n",
    "\n",
    "### Advantages of Bagging:\n",
    "1. **Reduces Overfitting**: Bagging helps in reducing overfitting in complex models like decision trees by averaging out errors across multiple models.\n",
    "2. **Stability**: The method improves the stability of the model by making it less sensitive to the variations in the data.\n",
    "3. **Parallelization**: Since each model is trained independently, bagging can be parallelized, leading to faster training times.\n",
    "\n",
    "### Disadvantages of Bagging:\n",
    "1. **Computation**: Bagging requires training multiple models, which can be computationally expensive, especially for large datasets.\n",
    "2. **Bias-Variance Trade-off**: While it reduces variance, Bagging does not address bias. If the base model is too biased (e.g., a very simple model), Bagging might not help much.\n",
    "\n",
    "### Bagging Variants:\n",
    "- **Random Forest**: A specific form of Bagging where, instead of using all features for each decision tree, a random subset of features is considered for each split in a decision tree. This further decorrelates the trees and improves performance.\n",
    "- **Bagging with Other Models**: Although decision trees are the most common base model for Bagging, it can also be applied with other models like **k-NN**, **SVMs**, or **Logistic Regression**.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary:\n",
    "- **Bagging** (Bootstrap Aggregating) is an ensemble method that trains multiple models on random subsets of the training data and aggregates their predictions to improve performance.\n",
    "- It is particularly effective with high-variance models like decision trees.\n",
    "- The key advantage of Bagging is its ability to reduce variance and overfitting, but it may come with increased computational cost.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of Bagging in Python using **scikit-learn**:\n",
    "You can use the `BaggingClassifier` or `BaggingRegressor` from **scikit-learn** to implement bagging in a classification or regression task.\n",
    "\n",
    "Hereâ€™s an example using **BaggingClassifier** with **DecisionTreeClassifier**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 96.30%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_wine\n",
    "\n",
    "# Load a sample dataset (Iris dataset)\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Load wine dataset\n",
    "wine = load_wine()\n",
    "X, y = wine.data, wine.target\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize a base classifier (Decision Tree)\n",
    "base_classifier = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Initialize BaggingClassifier with Decision Tree as base model\n",
    "bagging_model = BaggingClassifier(base_classifier, n_estimators=5, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "bagging_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = bagging_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((178, 13), (178,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine = load_wine()\n",
    "X, y = wine.data, wine.target\n",
    "X.shape, y.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
