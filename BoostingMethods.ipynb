{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting methods\n",
    "\n",
    "\n",
    "* AdaBoost\n",
    "* Gradient Boosting (Extreme Gradient Boosting)\n",
    "* CatBoost\n",
    "* XGBoost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaBoost (Adaptive Boosting) \n",
    "\n",
    "\n",
    "AdaBoost is an ensemble learning method that combines multiple weak learners to form a strong learner. It focuses on improving the accuracy of weak classifiers (often decision trees with a small depth, referred to as \"stumps\") by assigning higher weights to the misclassified samples and adjusting the model accordingly. AdaBoost is particularly effective for binary classification problems but can be extended to multi-class problems as well.\n",
    "\n",
    "### The main idea is:\n",
    "\n",
    "* Initially, all data points have equal weight.\n",
    "* In each iteration, the weak classifier is trained, and the misclassified points receive higher weights.\n",
    "* The weak learners are combined into a final strong learner by weighting them according to their accuracy.\n",
    "\n",
    "#### Steps in AdaBoost:\n",
    "1. **Initialize weights:** Assign equal weights to all data points in the training set.\n",
    "2. **Train a weak model:** Fit a simple model (weak learner), such as a decision tree, on the data.\n",
    "3. **Update weights:** Increase the weights of the misclassified data points so that the next model focuses more on them.\n",
    "4. **Repeat:** Train additional weak models, each focusing on the previous errors, until a pre-specified number of models is reached.\n",
    "\n",
    "5. **Combine models:** Combine the predictions of all models by weighted voting.\n",
    "\n",
    "#### AdaBoost Algorithm:\n",
    "* Initial weight of data points: All data points are assigned equal weight at the beginning.\n",
    "* Classifier weights: Classifiers that perform well are given higher weights, while those that perform poorly are given lower weights.\n",
    "* Final Prediction: The weighted sum of the predictions of each weak learner is used for the final classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 100.00%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       1.00      1.00      1.00         9\n",
      "           2       1.00      1.00      1.00        11\n",
      "\n",
      "    accuracy                           1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Adaboost with decision tree classifier\n",
    "\n",
    "# Import necessary libraries\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data  # Features\n",
    "y = iris.target  # Target labels\n",
    "\n",
    "\n",
    "\n",
    "# Split the dataset into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize a Decision Tree classifier (weak learner)\n",
    "dt_classifier = DecisionTreeClassifier(max_depth=1)\n",
    "\n",
    "# Initialize AdaBoost with the weak learner (Decision Tree)\n",
    "ada_boost = AdaBoostClassifier(base_estimator=dt_classifier, n_estimators=50, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "ada_boost.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = ada_boost.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advantages of AdaBoost:\n",
    "\n",
    "* **High Accuracy:** AdaBoost often results in strong performance, even with simple weak learners.\n",
    "* **Handles Overfitting:** AdaBoost can reduce the risk of overfitting when used with weak learners.\n",
    "* **Works Well for Binary Classification:** Although it can handle multi-class problems, it works especially well for binary classification tasks.\n",
    "* **No Hyperparameter Tuning Needed:** AdaBoost is simple to implement and usually doesn't require much parameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting \n",
    "\n",
    "Gradient Boosting is another powerful ensemble learning technique that builds a model by combining multiple weak learners to form a strong learner. Unlike AdaBoost, which adjusts weights of misclassified points, Gradient Boosting builds new models that correct the errors made by previous models by focusing on the residual errors (differences between the predicted and actual values).\n",
    "\n",
    "In Gradient Boosting, each new model is trained to minimize the residual errors of the previous model, which allows the model to focus on the areas where the previous model was weak. The name \"gradient\" refers to the gradient descent optimization used to minimize the loss function.\n",
    "\n",
    "### Key Concepts of Gradient Boosting:\n",
    "\n",
    "* Weak Learners: Like AdaBoost, Gradient Boosting often uses decision trees as weak learners, but in this case, trees are typically shallow (with a limited depth).\n",
    "* Residuals: Gradient Boosting fits the new model to the residual errors (or gradients of the loss function) from the previous model.\n",
    "* Loss Function: It uses a differentiable loss function (e.g., Mean Squared Error for regression, Log Loss for classification) to evaluate the model's performance.\n",
    "* Learning Rate: The contribution of each new tree to the overall model is scaled by a parameter called the learning rate. A smaller learning rate typically requires more trees but can help prevent overfitting.\n",
    "* Boosting Iterations: The number of boosting rounds (trees) that will be built is a hyperparameter, typically controlled by n_estimators in the implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.29\n",
      "R-squared: 0.78\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Load the California Housing dataset\n",
    "california_housing = fetch_california_housing()\n",
    "X = california_housing.data  # Features\n",
    "y = california_housing.target  # Target variable (median house value)\n",
    "\n",
    "# Split the dataset into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Gradient Boosting Regressor\n",
    "gb_regressor = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "gb_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = gb_regressor.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Mean Squared Error: {mse:.2f}\")\n",
    "print(f\"R-squared: {r2:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages of Gradient Boosting:\n",
    "\n",
    "* High Accuracy: Gradient Boosting can achieve very high predictive accuracy, often outperforming other models, especially when the model is properly tuned.\n",
    "* Flexible: It can be used for both regression and classification tasks.\n",
    "* Handles Complex Data: It can handle complex relationships and interactions between features, making it powerful for a wide variety of datasets.\n",
    "* Feature Importance: Gradient Boosting models provide insights into feature importance, which can help with feature selection.\n",
    "\n",
    "\n",
    "### Disadvantages of Gradient Boosting:\n",
    "1. **Overfitting:** Gradient Boosting models are prone to overfitting if the number of estimators is too high or if the learning rate is too low. Regularization techniques, such as limiting the tree depth or using subsampling, can help prevent this.\n",
    "2. **Training Time:** Gradient Boosting can be computationally expensive, especially with large datasets.\n",
    "3. **Sensitive to Hyperparameters:** The performance of Gradient Boosting is sensitive to hyperparameters like the learning rate, the number of estimators, and the depth of the trees. Careful tuning is necessary to avoid overfitting or underfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing AdaBoost and Gradient Boosting\n",
    "\n",
    "| Feature\t| AdaBoost\t| Gradient Boosting|\n",
    "|-----------|-----------|-----------|\n",
    "|Main Concept|\tFocuses on misclassified points, giving them more weight.\t|Focuses on residuals (errors) to minimize loss function.|\n",
    "|Learners\t|Weak learners (usually decision stumps).\t|Weak learners (usually shallow decision trees).|\n",
    "|Error Correction|\tCorrects by focusing on misclassified points.|\tCorrects by minimizing residual errors using gradient descent.\n",
    "|Handling Overfitting\t|Can overfit with noisy data. Regularization is less common.\t|Better regularization options, less prone to overfitting with tuning.|\n",
    "|Complexity\t|Simpler to train and faster to compute.\t|More computationally expensive and time-consuming.|\n",
    "|Sensitive to Noise\t|More sensitive to noisy data.|\tLess sensitive but still prone to overfitting.|\n",
    "|Flexibility|\tBest for binary classification.|\tWorks well for both classification and regression tasks.|\n",
    "|Hyperparameters|\tFewer hyperparameters to tune.\t|Requires more hyperparameter tuning, including tree depth and learning rate.|"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
