{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost (Extreme Gradient Boosting)\n",
    "XGBoost is an optimized, efficient, and scalable implementation of Gradient Boosting. It has gained immense popularity in machine learning competitions, including Kaggle, due to its high performance and speed. XGBoost improves upon traditional Gradient Boosting by introducing several enhancements that make it more powerful, faster, and easier to use for both classification and regression tasks.\n",
    "\n",
    "## Key Features of XGBoost:\n",
    "* **Efficiency:** XGBoost uses advanced techniques like parallel processing and tree pruning to speed up training.\n",
    "* **Regularization:** It includes L1 (Lasso) and L2 (Ridge) regularization, which helps prevent overfitting.\n",
    "* **Handling Missing Data:** XGBoost can handle missing data without requiring imputation.\n",
    "* **Tree Pruning:** Instead of growing trees to a fixed depth, XGBoost uses Max Depth and then prunes trees based on the Gamma parameter, ensuring that the model doesn't become too complex.\n",
    "* **Scalability:** XGBoost is highly scalable, which makes it suitable for large datasets.\n",
    "* **Flexibility:** It supports a wide range of loss functions for both regression and classification tasks.\n",
    "* **Cross-validation:** XGBoost offers built-in support for cross-validation during training, which helps with hyperparameter tuning.\n",
    "* **Early Stopping:** It supports early stopping to prevent overfitting and saves training time.\n",
    "\n",
    "### XGBoost Workflow:\n",
    "* **Boosting Process:** Similar to traditional Gradient Boosting, but optimized for speed and efficiency. Trees are built sequentially, and each tree tries to correct the mistakes (residuals) of the previous tree.\n",
    "* **Learning Rate (Eta):** A smaller learning rate helps prevent overfitting but requires more trees (iterations) to learn effectively.\n",
    "* **Gradient and Hessian:** XGBoost uses second-order gradient information (Hessian) in the optimization process, leading to faster and more accurate convergence compared to using only the first-order gradient.\n",
    "\n",
    "### Basic Steps in XGBoost:\n",
    "* **Data Preprocessing:** Preprocess your data (e.g., handle missing values, scale/normalize features if necessary).\n",
    "* **Set Hyperparameters:** Set XGBoost-specific parameters like learning_rate, n_estimators, max_depth, and subsample.\n",
    "* **Train the Model:** Train the model on the dataset.\n",
    "* **Evaluate the Model:** Use performance metrics like accuracy (classification) or RMSE (regression).\n",
    "* **Tune Hyperparameters:** Fine-tune the hyperparameters using cross-validation or grid search.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-2.1.4-py3-none-macosx_10_15_x86_64.macosx_11_0_x86_64.macosx_12_0_x86_64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: numpy in ./VENV/lib/python3.11/site-packages (from xgboost) (2.2.3)\n",
      "Requirement already satisfied: scipy in ./VENV/lib/python3.11/site-packages (from xgboost) (1.15.2)\n",
      "Downloading xgboost-2.1.4-py3-none-macosx_10_15_x86_64.macosx_11_0_x86_64.macosx_12_0_x86_64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xgboost\n",
      "Successfully installed xgboost-2.1.4\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./VENV/lib/python3.11/site-packages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Importing libraries\n",
    "import xgboost as xgb\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "data = load_iris()\n",
    "X = data.data  # Features\n",
    "y = data.target  # Target variable (class labels)\n",
    "\n",
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert the datasets into DMatrix format (optimized for XGBoost)\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "# Define the parameters for XGBoost (basic setup)\n",
    "params = {\n",
    "    'objective': 'multi:softmax',  # Multi-class classification (softmax)\n",
    "    'num_class': 3,               # Number of classes (3 for Iris dataset)\n",
    "    'max_depth': 4,               # Maximum depth of trees\n",
    "    'eta': 0.1,                   # Learning rate\n",
    "    'eval_metric': 'merror'       # Evaluation metric (multi-class error rate)\n",
    "}\n",
    "\n",
    "# Train the XGBoost model with 100 boosting rounds\n",
    "num_round = 100\n",
    "bst = xgb.train(params, dtrain, num_round)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = bst.predict(dtest)\n",
    "\n",
    "# Convert predictions into integers (class labels)\n",
    "y_pred = y_pred.astype(int)\n",
    "\n",
    "# Evaluate the model using accuracy score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaboost Vs XGBoost:\n",
    "\n",
    "|Feature|\tAdaBoost|\tXGBoost|\n",
    "|----|----|----|\n",
    "|Boosting Mechanism|\tFocuses on misclassified data points\t|Focuses on residuals using gradient descent|\n",
    "|Weak Learners|\tTypically uses decision stumps (shallow trees)|\tUses shallow decision trees, more flexible|\n",
    "|Regularization\t|None, prone to overfitting|\tBuilt-in L1 and L2 regularization to prevent overfitting|\n",
    "|Handling Missing Data|\tDoes not handle missing data directly\t|Can handle missing data naturally|\n",
    "|Performance|\tGood for smaller datasets, less computationally intensive\t|High performance and scalability, optimized for large datasets|\n",
    "|Hyperparameter Tuning|\tFew hyperparameters|\tMore hyperparameters to tune|\n",
    "|Overfitting\t|Prone to overfitting on noisy data\t|More robust to overfitting with regularization|\n",
    "|Computational Complexity\t|Simpler and faster for small datasets\t|Requires more computational resources, but scales better|\n",
    "|Interpretability\t|Easier to interpret, simple weak learners\t|More difficult to interpret, but can be analyzed with SHAP|\n",
    "|Use Case\t|Simple classification tasks, small datasets\t|Complex, large datasets, high-performance tasks|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt-text](images/Multiclass_ErrorRate.png 'Definitions')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CatBoost\n",
    "\n",
    "** CatBoost (Categorical Boosting)** is an open-source, high-performance gradient boosting algorithm developed by Yandex. It is designed to handle categorical features directly, without needing to manually encode them (like one-hot encoding or label encoding), which is one of its standout features. CatBoost is used for classification, regression, and ranking tasks, and it's known for being highly efficient, handling large datasets with high dimensionality, and providing state-of-the-art performance.\n",
    "\n",
    "## Key Features of CatBoost:\n",
    "\n",
    "**1.  Handling Categorical Features:**\n",
    "\n",
    "One of the major advantages of CatBoost is its ability to handle categorical features directly. Most machine learning models require converting categorical variables into numeric values (e.g., using one-hot encoding or label encoding). CatBoost, however, uses its own algorithm to process categorical features without the need for preprocessing them manually.\n",
    "\n",
    "**2. Gradient Boosting:**\n",
    "Like other gradient boosting algorithms (e.g., XGBoost and LightGBM), CatBoost builds an ensemble of trees sequentially. Each new tree tries to correct the errors (residuals) made by the previous trees in the ensemble. This helps in improving the overall model performance.\n",
    "\n",
    "**3. Efficient and Fast:**\n",
    "\n",
    "CatBoost uses ordered boosting, which helps reduce overfitting and provides better generalization, especially on smaller datasets. This ordered boosting process helps it make better use of categorical features.\n",
    "\n",
    "**4. Robust to Overfitting:**\n",
    "\n",
    "Thanks to its ordered boosting and regularization techniques, CatBoost is robust to overfitting, making it a good choice for datasets that have complex patterns or noise.\n",
    "\n",
    "**5. Supports Missing Values:**\n",
    "\n",
    "CatBoost automatically handles missing data during model training, so you don’t need to perform any special preprocessing steps to handle missing values.\n",
    "\n",
    "**6. Fast Training with Parallelization:**\n",
    "\n",
    "CatBoost is optimized for speed, thanks to parallelization and its ability to efficiently use hardware resources (e.g., multi-core CPUs or GPUs).\n",
    "\n",
    "**7. Great for Tabular Data:**\n",
    "\n",
    "CatBoost is particularly powerful for tabular data, where categorical variables and interactions between features are important.\n",
    "\n",
    "**8. Model Interpretation:**\n",
    "\n",
    "CatBoost offers tools for model interpretation and understanding feature importance, making it easier to explain and debug the model.\n",
    "\n",
    "### When to Use CatBoost:\n",
    "* **Categorical Data:** CatBoost is ideal when you have categorical features and don’t want to manually encode them.\n",
    "* **General Purpose:** It works well on a variety of tasks like classification, regression, and ranking.\n",
    "* **Complex, Noisy Data:** If your data has noise or complex relationships, CatBoost’s robustness to overfitting is beneficial.\n",
    "* **Scalability:** If you have large datasets and require fast, scalable solutions, CatBoost's parallelized training can be an advantage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary CatBoost\n",
    "\n",
    "* CatBoost actually divides a given dataset into random permutation. by default, Catboost creates four random permutations. With this randomness, we can further stop overfitting your model.\n",
    "We can further control this randomness by tuning parameter bagging_temperature.\n",
    "Note that the ordered boosting typically gets slower with small datasets (i.e., less than 50k samples), but it generally has very fast inference, because the algorithm uses specific kind of trees called symmetric trees.\n",
    "\n",
    "Catboost and XGBoost are slow with CPU training.\n",
    "Catboost doesnot perform well with sparse dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting catboost\n",
      "  Downloading catboost-1.2.7-cp311-cp311-macosx_11_0_universal2.whl.metadata (1.2 kB)\n",
      "Collecting graphviz (from catboost)\n",
      "  Downloading graphviz-0.20.3-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: matplotlib in ./VENV/lib/python3.11/site-packages (from catboost) (3.10.0)\n",
      "Collecting numpy<2.0,>=1.16.0 (from catboost)\n",
      "  Using cached numpy-1.26.4-cp311-cp311-macosx_10_9_x86_64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: pandas>=0.24 in ./VENV/lib/python3.11/site-packages (from catboost) (2.2.3)\n",
      "Requirement already satisfied: scipy in ./VENV/lib/python3.11/site-packages (from catboost) (1.15.2)\n",
      "Collecting plotly (from catboost)\n",
      "  Downloading plotly-6.0.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: six in ./VENV/lib/python3.11/site-packages (from catboost) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./VENV/lib/python3.11/site-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./VENV/lib/python3.11/site-packages (from pandas>=0.24->catboost) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./VENV/lib/python3.11/site-packages (from pandas>=0.24->catboost) (2025.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./VENV/lib/python3.11/site-packages (from matplotlib->catboost) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in ./VENV/lib/python3.11/site-packages (from matplotlib->catboost) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./VENV/lib/python3.11/site-packages (from matplotlib->catboost) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./VENV/lib/python3.11/site-packages (from matplotlib->catboost) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in ./VENV/lib/python3.11/site-packages (from matplotlib->catboost) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in ./VENV/lib/python3.11/site-packages (from matplotlib->catboost) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./VENV/lib/python3.11/site-packages (from matplotlib->catboost) (3.2.1)\n",
      "Collecting narwhals>=1.15.1 (from plotly->catboost)\n",
      "  Downloading narwhals-1.28.0-py3-none-any.whl.metadata (10 kB)\n",
      "Downloading catboost-1.2.7-cp311-cp311-macosx_11_0_universal2.whl (27.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.1/27.1 MB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached numpy-1.26.4-cp311-cp311-macosx_10_9_x86_64.whl (20.6 MB)\n",
      "Downloading graphviz-0.20.3-py3-none-any.whl (47 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.1/47.1 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading plotly-6.0.0-py3-none-any.whl (14.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.8/14.8 MB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading narwhals-1.28.0-py3-none-any.whl (308 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m308.9/308.9 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numpy, narwhals, graphviz, plotly, catboost\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.2.3\n",
      "    Uninstalling numpy-2.2.3:\n",
      "      Successfully uninstalled numpy-2.2.3\n",
      "Successfully installed catboost-1.2.7 graphviz-0.20.3 narwhals-1.28.0 numpy-1.26.4 plotly-6.0.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "dlopen(./VENV/lib/python3.11/site-packages/catboost/_catboost.so, 0x0002): symbol not found in flat namespace '_PyCMethod_New'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-ed9d6d7333b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Import necessary libraries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcatboost\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCatBoostClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_iris\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AcademicWork/Interview-Prep/personal-repo/ML_Algorithms/VENV/lib/python3.11/site-packages/catboost/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m from .core import (\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mFeaturesData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEFstrType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEShapCalcType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEFeaturesSelectionAlgorithm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEFeaturesSelectionGrouping\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mPool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCatBoost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCatBoostClassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCatBoostRegressor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCatBoostRanker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCatBoostError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_gaussian_process\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0msum_models\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_have_equal_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_regressor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_classifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_ranker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMultiRegressionCustomMetric\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mMultiRegressionCustomObjective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMultiTargetCustomMetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMultiTargetCustomObjective\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AcademicWork/Interview-Prep/personal-repo/ML_Algorithms/VENV/lib/python3.11/site-packages/catboost/core.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mplot_helpers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msave_plot_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtry_plot_offline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOfflineMetricVisualizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_catboost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBuiltinMetric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AcademicWork/Interview-Prep/personal-repo/ML_Algorithms/VENV/lib/python3.11/site-packages/catboost/plot_helpers.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_catboost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mfspath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_catboost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: dlopen(./VENV/lib/python3.11/site-packages/catboost/_catboost.so, 0x0002): symbol not found in flat namespace '_PyCMethod_New'"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the CatBoostClassifier\n",
    "model = CatBoostClassifier(iterations=500, depth=6, learning_rate=0.05, cat_features=[])\n",
    "\n",
    "# Fit the model to the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "dlopen(./VENV/lib/python3.11/site-packages/catboost/_catboost.so, 0x0002): symbol not found in flat namespace '_PyCMethod_New'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-e69177488928>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_wine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcatboost\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCatBoostClassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/AcademicWork/Interview-Prep/personal-repo/ML_Algorithms/VENV/lib/python3.11/site-packages/catboost/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m from .core import (\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mFeaturesData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEFstrType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEShapCalcType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEFeaturesSelectionAlgorithm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEFeaturesSelectionGrouping\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mPool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCatBoost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCatBoostClassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCatBoostRegressor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCatBoostRanker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCatBoostError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_gaussian_process\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0msum_models\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_have_equal_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_regressor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_classifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_ranker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMultiRegressionCustomMetric\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mMultiRegressionCustomObjective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMultiTargetCustomMetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMultiTargetCustomObjective\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AcademicWork/Interview-Prep/personal-repo/ML_Algorithms/VENV/lib/python3.11/site-packages/catboost/core.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mplot_helpers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msave_plot_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtry_plot_offline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOfflineMetricVisualizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_catboost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBuiltinMetric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AcademicWork/Interview-Prep/personal-repo/ML_Algorithms/VENV/lib/python3.11/site-packages/catboost/plot_helpers.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_catboost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mfspath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_catboost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: dlopen(./VENV/lib/python3.11/site-packages/catboost/_catboost.so, 0x0002): symbol not found in flat namespace '_PyCMethod_New'"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "from catboost import CatBoostClassifier, Pool, cv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
