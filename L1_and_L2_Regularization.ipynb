{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "198340ae",
   "metadata": {},
   "source": [
    "# Ridge Regression\n",
    "\n",
    "\n",
    "**Ridge regression** is a type of linear regression that addresses the issue of multicollinearity (when independent variables are highly correlated) and overfitting by adding a penalty term to the cost function. This penalty term discourages large coefficients, leading to more stable and generalizable models.\n",
    "\n",
    "In standard linear regression, the goal is to minimize the sum of squared errors (residuals) between the predicted values and actual values:\n",
    "\n",
    "![alt text](images/std_regression.png \"Standard Regression\")\n",
    "\n",
    "In ridge regression, we add a penalty term that is proportional to the sum of the squares of the coefficients:\n",
    "\n",
    "![alt text](images/ridge_regression.png \"Ridge Regression\")\n",
    "\n",
    "Here:\n",
    "\n",
    "* 𝜆 is the regularization parameter that controls the strength of the penalty. A higher λ results in more regularization, leading to smaller coefficients.\n",
    "* 𝛽𝑗 represents the coefficients of the model.\n",
    "* The first term is the usual least-squares error.\n",
    "* The second term is the ridge penalty that discourages large coefficients.\n",
    "\n",
    "### Key Points:\n",
    "* **Regularization:** Ridge regression applies L2 regularization, which penalizes the sum of the squared coefficients.\n",
    "* **Control Overfitting:** By shrinking the coefficients, ridge regression helps prevent overfitting, especially when there are many features or multicollinearity in the data.\n",
    "* **Solution:** Unlike standard linear regression, ridge regression tends to produce more stable models when there are many correlated features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4906ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf4e482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example for L2(Ridge) Regression\n",
    "\n",
    "# Importing necessary libraries\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Example dataset: predicting house prices based on features\n",
    "data = {\n",
    "    'bedrooms': [3, 4, 2, 5, 3, 4, 3, 2, 4, 3],\n",
    "    'square_feet': [1500, 2000, 1200, 2500, 1800, 2200, 1600, 1300, 1900, 1700],\n",
    "    'age': [10, 15, 7, 20, 12, 18, 9, 5, 16, 11],\n",
    "    'price': [400000, 500000, 350000, 600000, 450000, 480000, 420000, 380000, 490000, 460000]\n",
    "}\n",
    "\n",
    "# Converting to a pandas DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Features and target\n",
    "X = df[['bedrooms', 'square_feet', 'age']]  # Features\n",
    "y = df['price']  # Target\n",
    "\n",
    "# Split data into train and test sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features (important for ridge regression)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create and train the ridge regression model\n",
    "ridge_model = Ridge(alpha=1.0)  # alpha is the regularization strength (λ)\n",
    "ridge_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = ridge_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model using mean squared error (MSE)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)  # Root Mean Squared Error\n",
    "\n",
    "print(f\"Predicted Prices: {y_pred}\")\n",
    "print(f\"Actual Prices: {y_test.values}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6342390d",
   "metadata": {},
   "source": [
    "## Lasso Regression\n",
    "\n",
    "* **Lasso Regression** (Least Absolute Shrinkage and Selection Operator) is another type of linear regression that also aims to address overfitting and multicollinearity, like ridge regression. However, unlike ridge regression, which uses L2 regularization (squaring the coefficients), lasso regression uses L1 regularization (taking the absolute values of the coefficients). This difference has significant consequences for how the model behaves.\n",
    "\n",
    "![alt_text](images/lasso_regression.png \"Lasso Regression\")\n",
    "\n",
    "Where:\n",
    "\n",
    "* 𝑦i is the true value, and \n",
    "* 𝑦^𝑖 is the predicted value.\n",
    "* 𝛽𝑗 are the coefficients.\n",
    "* λ is the regularization parameter, controlling the strength of the penalty.\n",
    "* The first term is the usual least-squares error.\n",
    "* The second term is the L1 penalty (the sum of the absolute values of the coefficients).\n",
    "**Key Points:**\n",
    "* **L1 Regularization:** Lasso applies L1 regularization, which encourages sparsity (i.e., it forces some of the coefficients to become exactly zero).\n",
    "* **Feature Selection:** Since some coefficients are driven to zero, lasso regression can be useful for feature selection by eliminating irrelevant or redundant features.\n",
    "* **Overfitting Control:** Like ridge regression, lasso helps prevent overfitting, but it can also result in simpler, more interpretable models since irrelevant features are removed.\n",
    "* Choosing 𝜆, λ is typically selected through cross-validation.\n",
    "\n",
    "#### ** Differences Between Lasso and Ridge Regression:**\n",
    "* **Ridge:** Uses L2 regularization (penalizing the sum of squared coefficients). It usually keeps all features but shrinks their values.\n",
    "* **Lasso:** Uses L1 regularization (penalizing the sum of absolute values of coefficients). It tends to set some coefficients exactly to zero, effectively selecting a subset of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a5fbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Example dataset: predicting house prices based on features\n",
    "data = {\n",
    "    'bedrooms': [3, 4, 2, 5, 3, 4, 3, 2, 4, 3],\n",
    "    'square_feet': [1500, 2000, 1200, 2500, 1800, 2200, 1600, 1300, 1900, 1700],\n",
    "    'age': [10, 15, 7, 20, 12, 18, 9, 5, 16, 11],\n",
    "    'price': [400000, 500000, 350000, 600000, 450000, 480000, 420000, 380000, 490000, 460000]\n",
    "}\n",
    "\n",
    "# Converting to a pandas DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Features and target\n",
    "X = df[['bedrooms', 'square_feet', 'age']]  # Features\n",
    "y = df['price']  # Target\n",
    "\n",
    "# Split data into train and test sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features (important for lasso regression)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create and train the lasso regression model\n",
    "lasso_model = Lasso(alpha=0.1)  # alpha is the regularization strength (λ)\n",
    "lasso_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = lasso_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model using mean squared error (MSE)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)  # Root Mean Squared Error\n",
    "\n",
    "print(f\"Predicted Prices: {y_pred}\")\n",
    "print(f\"Actual Prices: {y_test.values}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f47127e",
   "metadata": {},
   "source": [
    "#### Explanation\n",
    "\n",
    "**Regularization Parameter** \n",
    "\n",
    "λ (or alpha):\n",
    "* **Low 𝜆:** If you set  λ to a very small value (or alpha close to 0), the model behaves similarly to linear regression (without regularization).\n",
    "* **High 𝜆:** If 𝜆 is large, the coefficients are heavily penalized, potentially making the model simpler and reducing overfitting, but it can also lead to underfitting if too many features are eliminated.\n",
    "\n",
    "**Key Differences from Ridge:**\n",
    "* Lasso can zero out coefficients, making it better for feature selection.\n",
    "* Ridge only shrinks coefficients but never eliminates them entirely."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
